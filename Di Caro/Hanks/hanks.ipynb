{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Hanks"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Il verbo transitivo che ho scelto è il verbo _raise_. Le frasi che compongono il corpus sono memorizzate nel file `sentences_raise.txt` e sono state prese da vari siti web."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Import"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.wsd import lesk\n",
    "from nltk.corpus import wordnet as wn\n",
    "import spacy\n",
    "from spacy.matcher import DependencyMatcher\n",
    "from collections import Counter"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Variabili globali"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Utilizziamo una variabile `special_cases_dict` che permette di assegnare il nome di un synset a una serie di parole speciali, come i pronomi, che non sono presenti in wordnet. Le parole sono rappresentate dalla coppia (tag, parola) che permette una migliore disambiguazione."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "metadata": {},
   "outputs": [],
   "source": [
    "special_cases_dict = {\n",
    "        'act.n.02': [('TO', 'to')],\n",
    "        'entity.n.01': [('DT', 'what'), ('WP', 'what'), ('NN', 'something'), ('NN', 'anything'), ('NN', 'everything'), \n",
    "                        ('DT', 'this'), ('WDT', 'this'), ('DT', 'these'), ('WDT', 'that'), ('DT', 'that'), ('DT', 'those')],\n",
    "        'person.n.01': [('PRP', 'i'), ('PRP', 'you'), ('PRP', 'he'), ('PRP', 'she'), \n",
    "                        ('PRP', 'we'), ('PRP', 'they'), ('PRP', 'him'), ('PRP', 'her'), \n",
    "                        ('PRP', 'them'), ('PRP', 'herself'), ('PRP', 'himself'), ('NNP', 'proper_name'),\n",
    "                        ('NN', 'anyone'), ('NN', 'proper_name'), ('PRP', 'themselves'), ('WP', 'whom')  ],\n",
    "        'animal.n.01': [('PRP', 'it'), ('PRP', 'itself'), ('NNS', 'koi')],\n",
    "        'artifact.n.01': [('WDT', 'which')],\n",
    "        'food.n.01': [('NNP', 'kosra')],\n",
    "        'number.n.02': [('CD', 'number')]\n",
    "    }"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Per il parsing delle frasi ho utilizzato il dependecyMatcher di spacy. Ho selezionato 3 pattern che permettono di riconoscere soggetto e oggetto del verbo. Ogni pattern è corroborato da una frase di esempio di quel pattern.\n",
    "\n",
    "C'è un quarto pattern che ho implementato e provato ad usare, ma questo causa problemi nell'estrazione di oggetto e soggetto del verbo da un match del dependency matcher ed inoltre si attiva per circa lo 0.6% delle frasi del corpus, e quindi è poco rilevante. I problemi derivano dal fatto che il quarto pattern abbia tipicamente soggetto e oggetto entrambi prima del verbo raise. L'estrazione di oggetto e soggetto da un match si aspetta, invece, che la prima parola del match sia il soggetto e l'ultima l'oggetto."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "metadata": {},
   "outputs": [],
   "source": [
    "nlp = spacy.load('en_core_web_sm')\n",
    "matcher = DependencyMatcher(nlp.vocab)\n",
    "\n",
    "# Example: I raised you better than this!\n",
    "# Pattern: raised --nsubj--> I\n",
    "#                 --pobj/dobj--> you\n",
    "pattern1 = [\n",
    "  {\n",
    "    \"RIGHT_ID\": \"anchor_raise\",\n",
    "    \"RIGHT_ATTRS\": {\"LEMMA\": \"raise\"}\n",
    "  },\n",
    "  {\n",
    "    \"LEFT_ID\": \"anchor_raise\",\n",
    "    \"REL_OP\": \">\",\n",
    "    \"RIGHT_ID\": \"subj\",\n",
    "    \"RIGHT_ATTRS\": {\"DEP\": \"nsubj\"}\n",
    "  },\n",
    "  {\n",
    "    \"LEFT_ID\": \"anchor_raise\",\n",
    "    \"REL_OP\": \">\",\n",
    "    \"RIGHT_ID\": \"obj\",\n",
    "    \"RIGHT_ATTRS\": {\"DEP\": {\"IN\": [\"pobj\", \"dobj\"]}}\n",
    "  },\n",
    "]\n",
    "\n",
    "# Example: He was raised by his grandparents.\n",
    "# Pattern: raised --nsubjpass--> He\n",
    "#                 --prep/agent--> by --pobj--> grandparents\n",
    "pattern2 = [\n",
    "  {\n",
    "    \"RIGHT_ID\": \"anchor_raise\",\n",
    "    \"RIGHT_ATTRS\": {\"LEMMA\": \"raise\"}\n",
    "  },\n",
    "  {\n",
    "    \"LEFT_ID\": \"anchor_raise\",\n",
    "    \"REL_OP\": \">\",\n",
    "    \"RIGHT_ID\": \"subj\",\n",
    "    \"RIGHT_ATTRS\": {\"DEP\": \"nsubjpass\"}\n",
    "  },\n",
    "  {\n",
    "    \"LEFT_ID\": \"anchor_raise\",\n",
    "    \"REL_OP\": \">\",\n",
    "    \"RIGHT_ID\": \"prep_or_agent\",\n",
    "    \"RIGHT_ATTRS\": {\"DEP\": {\"IN\": [\"prep\", \"agent\"]}}\n",
    "  },\n",
    "  {\n",
    "    \"LEFT_ID\": \"prep_or_agent\",\n",
    "    \"REL_OP\": \">\",\n",
    "    \"RIGHT_ID\": \"obj\",\n",
    "    \"RIGHT_ATTRS\": {\"DEP\": \"pobj\"}\n",
    "  },\n",
    "]\n",
    "\n",
    "# Example: You need to raise your hand.\n",
    "# Pattern: need --?--> raise --dobj--> hand\n",
    "#               --nsubj--> you\n",
    "pattern3 = [\n",
    "  {\n",
    "    \"RIGHT_ID\": \"anchor_verb\",\n",
    "    \"RIGHT_ATTRS\": {\"POS\": \"VERB\"}\n",
    "  },\n",
    "  {\n",
    "    \"LEFT_ID\": \"anchor_verb\",\n",
    "    \"REL_OP\": \">\",\n",
    "    \"RIGHT_ID\": \"raise\",\n",
    "    \"RIGHT_ATTRS\": {\"LEMMA\": \"raise\"}\n",
    "  },\n",
    "  {\n",
    "    \"LEFT_ID\": \"anchor_verb\",\n",
    "    \"REL_OP\": \">\",\n",
    "    \"RIGHT_ID\": \"nsubj\",\n",
    "    \"RIGHT_ATTRS\": {\"DEP\": \"nsubj\"}\n",
    "  },\n",
    "  {\n",
    "    \"LEFT_ID\": \"raise\",\n",
    "    \"REL_OP\": \">\",\n",
    "    \"RIGHT_ID\": \"obj\",\n",
    "    \"RIGHT_ATTRS\": {\"DEP\": \"dobj\"}\n",
    "  }\n",
    "]\n",
    "\n",
    "# PATTERN COMMENTATO PERCHE' CAUSA PROBLEMI NELL'ESTRAZIONE DI SUBJ E OBJ ED E' UTILIZZATO SU POCHE FRASI\n",
    "# # Example: The money they raised were used to buy some books.\n",
    "# # Pattern: raised --nsubj--> they\n",
    "# #                 --?--> money\n",
    "# pattern4 = [\n",
    "#   {\n",
    "#     \"RIGHT_ID\": \"anchor_raise\",\n",
    "#     \"RIGHT_ATTRS\": {\"LEMMA\": \"raise\"}\n",
    "#   },\n",
    "#   {\n",
    "#     \"LEFT_ID\": \"anchor_raise\",\n",
    "#     \"REL_OP\": \">\",\n",
    "#     \"RIGHT_ID\": \"subj\",\n",
    "#     \"RIGHT_ATTRS\": {\"DEP\": \"nsubj\"}\n",
    "#   },\n",
    "#   {\n",
    "#     \"LEFT_ID\": \"anchor_raise\",\n",
    "#     \"REL_OP\": \"<\",\n",
    "#     \"RIGHT_ID\": \"obj\",\n",
    "#     \"RIGHT_ATTRS\": {\"POS\": {\"IN\": [\"NOUN\", \"PRON\"]}}\n",
    "#   },\n",
    "# ]\n",
    "\n",
    "matcher.add(\"pattern1\", [pattern1])\n",
    "matcher.add(\"pattern2\", [pattern2])\n",
    "matcher.add(\"pattern3\", [pattern3])\n",
    "# matcher.add(\"pattern4\", [pattern4])"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Recupero delle frasi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of sentences with verb \"raise\" is: 693\n"
     ]
    }
   ],
   "source": [
    "def retrieve_sentences(read_path):\n",
    "    sentences = []\n",
    "    with open(read_path, mode='r', encoding='utf8') as f:\n",
    "        for sent in f.readlines():\n",
    "            sentences.append(sent.strip('\\n'))\n",
    "    return sentences, len(sentences)\n",
    "    \n",
    "path = 'resources/sentences_raise.txt'\n",
    "corpus, n = retrieve_sentences(path)\n",
    "print(f'Number of sentences with verb \"raise\" is: {n}')"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Processing"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A questo punto processiamo il corpus di frasi per cercare i pattern che abbiamo definito prima, estraendo oggetto e soggetto del verbo raise."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_corpus(corpus):\n",
    "    result = list()\n",
    "    for i, sent in enumerate(corpus):\n",
    "        subj, obj, subj_tag, obj_tag = get_subj_and_obj(sent)\n",
    "        if subj == '' or obj == '':\n",
    "            continue\n",
    "        subj_sense, obj_sense = retrieve_supersenses(subj, obj, subj_tag, obj_tag, sent)\n",
    "        tmp = {\n",
    "            'subj': subj,\n",
    "            'subj_sense': subj_sense,\n",
    "            'obj': obj,\n",
    "            'obj_sense': obj_sense\n",
    "        }\n",
    "        result.append(tmp)\n",
    "    return result\n",
    "        \n",
    "def get_subj_and_obj(sent):\n",
    "    sent = nlp(sent)\n",
    "    matches = matcher(sent)\n",
    "    for match in matches:\n",
    "        match_words = sorted(match[1])\n",
    "        phrase = sent[match_words[0]:match_words[len(match_words)-1]+1]\n",
    "        subj = phrase[0].text\n",
    "        dobj = phrase[len(phrase)-1].text\n",
    "        \n",
    "        return subj,dobj,phrase[0].tag_,phrase[len(phrase)-1].tag_\n",
    "    return '', '', '', ''\n",
    "\n",
    "def retrieve_supersenses(subj, obj, subj_tag, obj_tag, sent):\n",
    "    words = [subj, obj]\n",
    "    tags = [subj_tag, obj_tag]\n",
    "    senses = []\n",
    "    for i in range(len(words)):\n",
    "        sense = get_sense(words[i], tags[i], sent)\n",
    "        if sense is None:\n",
    "            senses.append('unknown')\n",
    "        elif sense.lexname() == 'noun.Tops' and sense.name() != 'entity.n.01':\n",
    "            w, _, _ = sense.name().split('.')\n",
    "            senses.append(\"noun.\" + w)\n",
    "        else:\n",
    "            senses.append(sense.lexname())\n",
    "    return senses\n",
    "\n",
    "def get_sense(word, tag, sent):\n",
    "    special = special_cases(word, tag)\n",
    "    if special is not None:\n",
    "        return special\n",
    "    return lesk(sent, word)\n",
    "\n",
    "def special_cases(word, tag):\n",
    "    if tag in ['NNP', 'NN'] and word[0].isupper():\n",
    "        word = 'proper_name'\n",
    "    if tag == 'CD':\n",
    "        word = 'number'\n",
    "    for k, v in special_cases_dict.items():\n",
    "        if (tag, word.lower()) in v:\n",
    "            return wn.synset(k)\n",
    "    return None\n",
    "\n",
    "obj_subj_list = process_corpus(corpus)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Risultati"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Nell'esposizione dei risultati la prima cosa da verificare è quante frasi i pattern utilizzati siano stati in grado di catturare. Vediamo quindi che la percentuale è poco superiore al 50%. Aggiungendo altri pattern è possibile aumentarla. Questo però richiederebbe un lavoro lungo e poco utile ai fini dell'esercizio, pertanto si è scelto di limitarsi ai tre pattern sopra presentati."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Percentuale di frasi nel corpus che sono state processate: 53.82%\n"
     ]
    }
   ],
   "source": [
    "def aggregate_results(obj_subj_list):\n",
    "    couples = []\n",
    "    for sent in obj_subj_list:\n",
    "        tmp = (sent['subj_sense'], sent['obj_sense'])\n",
    "        couples.append(tmp)\n",
    "    counter = Counter(couples)\n",
    "    sorted_counter = dict(sorted(counter.items(), key=lambda x: x[1], reverse=True))\n",
    "    return sorted_counter\n",
    "\n",
    "def count_used_sentences(counter):\n",
    "    sum = 0\n",
    "    for v in counter.values():\n",
    "        sum += v\n",
    "    return sum\n",
    "\n",
    "counter = aggregate_results(obj_subj_list)\n",
    "l = count_used_sentences(counter)\n",
    "perc_sent_used = round(l/n*100, 2)\n",
    "print(f'Percentuale di frasi nel corpus che sono state processate: {perc_sent_used}%')"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A questo punto analizziamo i risultati che abbiamo ottenuto, cercando di ignorare quelli meno rilevanti."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Of 147 total pairs, the 129 who occured in less that 4 different sentences were deleted\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{('noun.person', 'noun.person'): 52,\n",
       " ('noun.person', 'noun.possession'): 24,\n",
       " ('noun.person', 'noun.artifact'): 19,\n",
       " ('noun.person', 'noun.cognition'): 16,\n",
       " ('noun.person', 'noun.act'): 16,\n",
       " ('noun.person', 'verb.competition'): 15,\n",
       " ('noun.person', 'verb.communication'): 8,\n",
       " ('noun.person', 'noun.group'): 8,\n",
       " ('noun.person', 'noun.quantity'): 7,\n",
       " ('noun.person', 'noun.attribute'): 7,\n",
       " ('noun.person', 'noun.animal'): 7,\n",
       " ('noun.person', 'noun.communication'): 6,\n",
       " ('noun.communication', 'verb.communication'): 4,\n",
       " ('noun.group', 'noun.communication'): 4,\n",
       " ('noun.person', 'noun.relation'): 4,\n",
       " ('noun.person', 'adj.all'): 4,\n",
       " ('noun.artifact', 'noun.person'): 4,\n",
       " ('noun.person', 'verb.contact'): 4}"
      ]
     },
     "execution_count": 121,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def delete_less_frequent(counter, threshold):\n",
    "    new_counter = counter.copy()\n",
    "    count = 0\n",
    "    for key, value in counter.items():\n",
    "        if value < threshold:\n",
    "            count += 1\n",
    "            del new_counter[key]\n",
    "    return new_counter, count\n",
    "\n",
    "threshold = 4\n",
    "results, n_deleted = delete_less_frequent(counter, threshold)\n",
    "print(f'Of {len(counter)} total pairs, the {n_deleted} who occured in less that {threshold} different sentences were deleted')\n",
    "results"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Nell'analizzare i risultati è evidente come la grande maggioranza delle volte il soggetto accettato dal verbo \"raise\" è una persona, o un gruppo di individui. Il complemento oggetto, invece può variare in modo molto più libero tra i vari supersensi di WordNet.\n",
    "\n",
    "Vediamo che la prima coppia di supersensi è (noun.person, noun.person). Questo corrisponde all'utilizzo del termine raise con il significato di _crescere/allevare/educare_, come per esempio _\"He raised that child well\"_. La seconda, invece, è (noun.person, noun.possession) che corrisponde al significato di _fare una raccolta fondi_, come nella frase _\"The students are raising money for their school\"_. La terza coppia, invece, è (noun.person, noun.artifact) che corrisponde al significato di _sollevare_ all'utilizzo in frasi come _\"He raised the torch in order to better see in the darkness\"_. La quarta coppia, (noun.person, noun.cognition), corrisponde al significato di _fare una domanda_/_portare all'attenzione_ come nella frase _\"He raised a question\"_.\n",
    "\n",
    "E' interessante notare come per la prima coppia il significato espresso poc'anzi non sia l'unico: per esempio nell'ambito della danza, avrebbe senso dire _\"He raised her over his shoulders\"_, dove il verbo _raise_ non ha il significato di _crescere_, ma di _sollevare_."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "f51c5fe9e9a58efa1043b1548cfcb156c8f39c9e60365a17bc19defd3d30b528"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
