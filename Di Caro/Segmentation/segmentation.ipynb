{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Segmentation"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Descrizione dell'algoritmo utilizzato"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Input e output"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "L'input è composto da:\n",
    "- il documento, cioè una singola stringa composta da frasi concatenate senza alcun capoverso (per la costruzione esatta si rimanda a `build_input_file.ipynb`)\n",
    "- il numero di paragrafi che vogliamo individuare nel documento\n",
    "\n",
    "L'output dell'algoritmo è una piccola lista contenete le posizioni dei tagli tra un paragrafo e l'altro."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Iperparametri"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ci sono due iperparametri il cui scopo verrà chiarito commentando direttamente il codice. Essi sono:\n",
    "- numero di parole più frequenti;\n",
    "- span per l'estrazione dei risultati."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Ipotesi fatte"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "L'algoritmo che ho implementato si basa su alcune ipotesi:\n",
    "1. Ogni paragrafo avrà un suo argomento, che sarà sufficientemente diverso dall'argomento degli altri paragrafi. In seguito verrà chiarito cosa si intende esattamente con \"sufficientemente diverso\". \n",
    "2. Si possono utilizzare le parole più frequenti (parole-argomento) per individuare gli argomenti di ciascun paragrafo. In particolare, quindi, esiste una corrispondenza tra i paragrafi e i sottoinsiemi di una specifica partizione dell'insieme delle parole più frequenti. \n",
    "3. Si può utilizzare la presenza o l'assenza delle parole-argomento in una frase per determinare a quale argomento (e quindi paragrafo) appartiene quella frase. In particolare le frasi che contengono parole-argomento saranno dette \"immediatamente classificabili\"; viceversa quelle che non ne contengono alcuna saranno dette \"non immediatamente classificabili\" o, per brevità \"non classificabili\".\n",
    "4. Una frase non immediatamente classificabile, apparterrà al paragrafo \"più vicino\", cioè al paragrafo di appartenenza della frase immediatamente classificabile più vicina.\n",
    "\n",
    "L'ipotesi 4 è senza dubbio la più forte tra quelle fatte, e ha un effetto molto importante anche sulla correttezza dell'algoritmo: difficilmente questo algoritmo restituirà la posizione esatta dei tagli tra un paragrafo e l'altro, ma ne riuscità a dare la posizione approssimativa. Questa posizione non mi aspetto che sia troppo lontana da quella effettiva perché l'ipotesi 4 è vera se ci troviamo all'interno di un paragrafo e non sui bordi. Pertanto allontanadosi dai tagli effettivi l'applicazione dell'ipotesi 4 non introduce alcuna approssimazione."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Spiegazione dell'algoritmo"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "L'algoritmo si divide in più fasi:\n",
    "1. Individuazione degli argomenti di cui si parla nel documento di input sfruttando le ipotesi 1 e 2. Questa fase si divide a sua volta in:\n",
    "   1.  Individuazione delle parole-argomento;\n",
    "   2.  Ottenimento degli embedding di tali parole;\n",
    "   3.  Clustering con KMeans sugli embedding delle parole-argomento;\n",
    "2. Classificazione delle frasi del documento basandosi su ipotesi 3 e 4. Avremo quindi rispettivamente:\n",
    "   1. Classificazione per presenza: per ogni frase viene calcolata la sua sovrapposizione con le parole-argomento di ogni cluster. La frase viene quindi classificata con il cluster che massimizza la sovrapposizione.\n",
    "   2. Classificazione per prossimità: per ogni frase non immediatamente classificabile, le si assegna il numero di paragrafo della frase immediatamente classificabile più vicina.\n",
    "3. Estrazione dei tagli tramite una specie di derivata. Per una spiegazione dettagliata si rimanda all'implementazione."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Inizializzazione"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Import"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "metadata": {},
   "outputs": [],
   "source": [
    "from gensim.test.utils import simple_preprocess\n",
    "from nltk.corpus import stopwords\n",
    "from sklearn.cluster import KMeans\n",
    "from collections import Counter\n",
    "from statistics import mean\n",
    "import json\n",
    "import pprint"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Recupero del documento di input"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Come accennato prima, la costruzione di questo documento avviene in `buil_input_file.ipynb`. Sono state prese tre pagine di Wikipedia (Film, Therapy, Napeoleon Bonaparte) da cui sono stati estratti 5 paragrafi, con frasi di più di 100 caratteri l'uno, che sono stati accorpati in un'unica riga. Sono state anche salvate le posizioni (in termini di frasi) dei tagli corretti."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 148,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Numero di frasi nel documento: 79\n"
     ]
    }
   ],
   "source": [
    "def retrieve_data(path):\n",
    "    data = ''\n",
    "    correct_cuts = []\n",
    "    with open(path, 'r', encoding='utf8') as f:\n",
    "        data = f.readline().strip('\\n')\n",
    "        correct_cuts = [int(x) for x in f.readline().strip('\\n').split(' ')]\n",
    "    return data.split('.'), correct_cuts\n",
    "\n",
    "input_data, correct_cuts = retrieve_data('resources/input_data.txt')\n",
    "print(f'Numero di frasi nel documento: {len(input_data)}')"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Recupero degli embeddings"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Gli embedding salvati nel file `resources/embeddings.json` sono stati estratti dal modello `'fasttext-wiki-news-subwords-300'` di gensim. L'estrazione avviene sempre in `buil_input_file.ipynb`. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 135,
   "metadata": {},
   "outputs": [],
   "source": [
    "def in_model(word):\n",
    "    try:\n",
    "        word_embeddings_model[word]\n",
    "    except KeyError:\n",
    "        return False\n",
    "    return True\n",
    "\n",
    "def get_embeddings(path):\n",
    "    embeddings = []\n",
    "    with open(path, 'r', encoding='utf8') as f:\n",
    "        embeddings = json.load(f)\n",
    "    return embeddings\n",
    "\n",
    "word_embeddings_model = get_embeddings('resources/embeddings.json')"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Esecuzione"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.1 Individuazione parole-argomento"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Qui introduciamo il primo iperparametro: il numero delle parole più frequenti. Io ho scelto 9 in quanto questo permette intuitivamente di avere 3 parole-argomento per ogni paragrafo."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 136,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Most frequent words: ['film', 'napoleon', 'trailer', 'care', 'therapy', 'corsican', 'shown', 'year', 'french']\n"
     ]
    }
   ],
   "source": [
    "def build_freq_dict(input_doc):\n",
    "    frequency_dict = {}\n",
    "    prep_input_doc = simple_preprocess(\". \".join(input_doc))\n",
    "    for word in prep_input_doc:\n",
    "        if word not in stopwords.words(\"english\"):\n",
    "            if is_plural(word):\n",
    "                word = to_singular(word)\n",
    "            if word not in frequency_dict.keys():\n",
    "                frequency_dict[word] = 0\n",
    "            frequency_dict[word] += 1\n",
    "    return frequency_dict\n",
    "\n",
    "def is_plural(word):\n",
    "    return word[-1] == 's' or word[-2:] == 'es'\n",
    "\n",
    "def to_singular(word):\n",
    "    if word[-2:] == 'es':\n",
    "        return word[:-2]\n",
    "    if  word[-1] == 's':\n",
    "        return word[:-1]\n",
    "    return word\n",
    "\n",
    "def get_most_frequent_words(frequency_dict, n_words):\n",
    "    freq_list = sorted(frequency_dict, key=frequency_dict.get, reverse=True)\n",
    "    freq_list = list(filter(lambda x: in_model(x), freq_list))\n",
    "    return freq_list[:n_words]\n",
    "\n",
    "N_MOST_FREQUENT_WORDS = 9\n",
    "freq_dict = build_freq_dict(input_data)\n",
    "most_freq_words = get_most_frequent_words(freq_dict, N_MOST_FREQUENT_WORDS)\n",
    "print(f'Most frequent words: {most_freq_words}')"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.2 Ottenimento degli embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 137,
   "metadata": {},
   "outputs": [],
   "source": [
    "def retrieve_embeddings(words):\n",
    "    res = []\n",
    "    for w in words:\n",
    "        res.append(word_embeddings_model[w])\n",
    "    return res\n",
    "\n",
    "embeddings = retrieve_embeddings(most_freq_words)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.3 Clustering con KMeans"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Qui notiamo che il numero di cluster che vogliamo ottenere da KMeans coincide al numero di paragrafi che stiamo cercando nel testo."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 138,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Clusters are:\n",
      "{0: ['napoleon', 'corsican', 'french'],\n",
      " 1: ['care', 'therapy'],\n",
      " 2: ['film', 'trailer', 'shown', 'year']}\n"
     ]
    }
   ],
   "source": [
    "def cluster_frequent_words(embeddings, n_clust):\n",
    "    model = KMeans(n_clusters=n_clust)\n",
    "    model.fit(embeddings)\n",
    "    return model\n",
    "\n",
    "def extract_results(km, words):\n",
    "    result = dict((label, []) for label in set(km.labels_))\n",
    "    for i, label in enumerate(km.labels_):\n",
    "        result[label].append(words[i])\n",
    "    return result\n",
    "\n",
    "N_SEGMENTS = 3\n",
    "km = cluster_frequent_words(embeddings, N_SEGMENTS)\n",
    "clusters = extract_results(km, most_freq_words)\n",
    "print(\"Clusters are:\")\n",
    "pprint.pprint(clusters)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.1 Classificazione per presenza"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 149,
   "metadata": {},
   "outputs": [],
   "source": [
    "def occurrence_classification(input_data, clusters):\n",
    "    classification_list = [-1]*len(input_data)\n",
    "    for i, sent in enumerate(input_data):\n",
    "        max_clust = find_max_clust(sent.split(), clusters)\n",
    "        classification_list[i] = max_clust\n",
    "    return classification_list\n",
    "\n",
    "def find_max_clust(sentence, clusters):\n",
    "    sent = set(to_singular(word).lower() for word in sentence)\n",
    "    max_clust = -1\n",
    "    max_sovr = 0\n",
    "    for clust, synonyms in clusters.items():\n",
    "        sovr = len(sent & set(synonyms))\n",
    "        if sovr > max_sovr:\n",
    "            max_clust = clust\n",
    "            max_sovr = sovr\n",
    "    return max_clust"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.2 Classificazione per prossimità"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 140,
   "metadata": {},
   "outputs": [],
   "source": [
    "def proximity_classification(classification_list):\n",
    "    new_list = classification_list[:]\n",
    "    for sent, clust in enumerate(classification_list):\n",
    "        if clust != -1:\n",
    "            continue\n",
    "        closest_clust = find_closest_clust(sent, classification_list)\n",
    "        new_list[sent] = closest_clust\n",
    "    return new_list\n",
    "\n",
    "def find_closest_clust(position, classification_list):\n",
    "    def compute_distance(start, end, step):\n",
    "        for i in range(start, end, step):\n",
    "            if classification_list[i] != -1:\n",
    "                return i - position\n",
    "        return len(classification_list)+1\n",
    "  \n",
    "    left_distance = compute_distance(position-1,-1,-1)\n",
    "    right_distance = compute_distance(position+1,len(classification_list),1)\n",
    "\n",
    "    if abs(right_distance) <= abs(left_distance):\n",
    "        return classification_list[right_distance+position]\n",
    "    else:\n",
    "        return classification_list[left_distance+position]"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Risultato della classificazione"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Presentiamo i risultati della classificazione fatta finora come una lista la cui lunghezza è pari al numero delle frasi e i cui elementi sono il numero del cluster di riferimento per l'argomento assegnato a un paragrafo. Quindi in sostanza numeri diversi simboleggiano i diversi paragrafi."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 141,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Classification list: [2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 1, 1, 1, 1, 1, 1, 1, 1, 2, 2, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 2, 2, 0, 0, 0, 0, 2, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n"
     ]
    }
   ],
   "source": [
    "def classify_data(input_data, clusters):\n",
    "    classification_list = occurrence_classification(input_data, clusters)\n",
    "    classification_list = proximity_classification(classification_list)\n",
    "    return classification_list\n",
    "\n",
    "classification_list = classify_data(input_data, clusters)\n",
    "print(f'Classification list: {classification_list}')"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3. Estrazione dei tagli con la \"derivata\""
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Come si può vedere dai risultati della classificazione, la lista `classification_list` ha quasi la struttura di una funzione a gradini. Per esempio, sarà costantenemnte 2 per tutte le frasi che appartengono al paragrafo 2, mentre non appena entriamo nel paragrafo 1 la funzione diventa costantemente 1. C'è però un problema: all'interno dei paragrafi abbiamo dei picchi, quindi variazioni a questo pattern che rappresentano degli errori di classificazione. Per poter estrarre i tagli è necessario distinguere questi picchi dai gradini e qui introduciamo un concetto che prende ispirazione dalla derivata delle funzioni.\n",
    "\n",
    "In particolare, un picco si distingue da un gradino osservandone un certo intorno. Per esempio, se ho questa sequenza: `1,1,0,1,1` lo zero è chiaramente un picco, perché poi la funzione ritorna a valere 1. Tipicamente nello studio delle funzioni continue per studiare un certo intorno di un punto si utilizza la derivata, che è il limite del rapporto incrementale. In questo caso la funzione è discreta e quindi possiamo basarci solamente sulle differenze tra gli estremi di un intorno sempre più piccolo. In particolare stabiliamo una finestra di valori, la cui dimensione sarà data dall'iperparametro SPAN, e facciamo la medie delle differenze delle coppie opposte. Quindi per esempio nella lista `[1,1,0,0,0]` supponendo una finestra con span 2, la posizione 2 avrà una pseudo-derivata di $ \\frac{1-0 + 1-0}{2} = 1 $. Mentre la stessa posizione nella lista `[1,1,0,1,1]` avrà una pseudo-derivata di $ \\frac{1-1 + 1-1}{2} = 0 $.\n",
    "\n",
    "Utilizzando questo meccanismo siamo in grado di distinguere facilmente i picchi dai gradini della funzione perché i primi saranno caratterizzati da pseudo-derivata < 1, mentre gli ultimi l'avranno esattamente uguale a 1.\n",
    "\n",
    "Tutto quanto detto finora funziona solo se la lista è composta unicamente da 0 e da 1. Se introduciamo anche altri numeri l'estrazione dei picchi diventa immensamente più complicata. Pertanto `classification_list` deve essere modificata. Da notare che questa modifica causa una perdita dell'informazione, ma è informazione che non è rilevante ai fini dell'output dell'algoritmo. Infatti ci interessa unicamente individuare le posizioni dei tagli, e non stabilire l'ordine degli argomenti affrontati dal documento. Pertanto operiamo anzitutto una trasformazione della lista in termini di 0 e 1, per poi effettuare l'estrazione dei tagli utilizzando la pseudo-derivata descritta sopra.\n",
    "\n",
    "__Iperparametro SPAN:__ la sua definizione è fondamentale per la buona estrazione dei tagli. In particolare i picchi vengono correttamente identificati solo se hanno una lunghezza inferiore alla metà dello span. Ho inserito un meccanismo di controllo che conti gli 1 della pseudo-derivata e faccia sapere all'utente se questi sono più del numero di tagli da lui specificato."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 146,
   "metadata": {},
   "outputs": [],
   "source": [
    "def derivative_based_extraction(classification_list, span, n_segments):\n",
    "    paragraph_list = transform_in_paragraph(classification_list)\n",
    "    derivative = build_derivative(paragraph_list, span)\n",
    "    count = Counter(derivative)\n",
    "    if count[1] > n_segments-1:\n",
    "        print(\"The extraction of cuts is uncertain. Try with a larger span.\")\n",
    "    positions = []\n",
    "    for i, el in enumerate(derivative):\n",
    "        if el == 1:\n",
    "            positions.append(i)\n",
    "    return positions\n",
    "\n",
    "def transform_in_paragraph(classification_list):\n",
    "    result = [0]\n",
    "    for i in range(1,len(classification_list)):\n",
    "        if classification_list[i] == classification_list[i-1]:\n",
    "            result.append(result[-1])\n",
    "        else:\n",
    "            result.append(int(not result[-1]))\n",
    "    return result\n",
    "\n",
    "def build_derivative(classification_list, span):\n",
    "    derivative = []\n",
    "    p = int(span/2)\n",
    "    for i in range(p,len(classification_list)-p):\n",
    "        window = classification_list[i-p:i+p]\n",
    "        diff = []\n",
    "        for j in range(p):\n",
    "            diff.append(window[j]-window[-(j+1)])\n",
    "        derivative.append(abs(mean(diff)))\n",
    "    return derivative"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 147,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cuts found by the algorithm: [25, 43]\n",
      "Correct cuts: [30, 47]\n"
     ]
    }
   ],
   "source": [
    "SPAN = 10\n",
    "cuts = derivative_based_extraction(classification_list, SPAN, N_SEGMENTS)\n",
    "print(f'Cuts found by the algorithm: {cuts}')\n",
    "print(f'Correct cuts: {correct_cuts}')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "f51c5fe9e9a58efa1043b1548cfcb156c8f39c9e60365a17bc19defd3d30b528"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
